{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# train = pd.read_csv('./data/train.csv')\n",
    "# test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_rows = train[train.isnull().any(axis=1)]\n",
    "\n",
    "# print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413f806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train.dropna(inplace=True)\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfaa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split()))\n",
    "# train['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split()))\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.kdeplot(train['Num_words_ST'], shade=True, color=\"#6c8ebf\", label='Selected Text')\n",
    "# sns.kdeplot(train['Num_word_text'], shade=True, color=\"#82B366\", label='Text')\n",
    "# plt.title('Distribution of words number',fontsize=14, fontweight='bold', position=(0.20, 1.0+0.05))\n",
    "# plt.xlabel('Words Number',fontsize=14)\n",
    "# plt.ylabel('Density',fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(columns=['text'], inplace=True)\n",
    "# train.rename(columns={'selected_text': 'text'}, inplace=True)\n",
    "\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_temp, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "# train, validation = train_test_split(train_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# train.to_csv(\"./dataset/train.csv\", index=False)\n",
    "# validation.to_csv(\"./dataset/validation.csv\", index=False)\n",
    "# test.to_csv(\"./dataset/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./dataset/train.csv\")\n",
    "test = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation = pd.read_csv(\"./dataset/validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ffbcb",
   "metadata": {},
   "source": [
    "# Funnel-Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abe6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "\n",
    "temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
    "colors = ['#B3B3B3','#6c8ebf', '#82B366']\n",
    "\n",
    "fig = go.Figure(go.Funnelarea(\n",
    "    text =temp.sentiment,\n",
    "    values = temp.text,\n",
    "    title = {\"position\": \"top center\", \"text\": \"Sentiment Distribution\"},\n",
    "    marker=dict(colors=colors)  \n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf122122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "more_stopwords = {'u', \"im\", \"day\"}\n",
    "stop_words  = stop_words.union(more_stopwords)    \n",
    "    \n",
    "# Basic text cleaning \n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                                         # Convert to lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)                               # Remove text in square bracket   \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)                 # Remove links\n",
    "    text = strip_html(text)                                          # Remove HTML tags\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "    text = re.sub('\\n', '', text)                                    # Remove line breaks\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)                              # Remove characters contain digits\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a28cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train['tokens'] = train['text'].apply(preprocess_text)\n",
    "test['tokens'] = test['text'].apply(preprocess_text)\n",
    "validation['tokens'] = validation['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Concatenate tokens for Word2Vec training\n",
    "all_tokens = pd.concat([train['tokens'], test['tokens'], validation['tokens']], axis=0)\n",
    "\n",
    "# sg: 1-skip-gram; 0(default)-CBOW\n",
    "# hs: 1-hierarchica softmax; 0(default)-negative sampling\n",
    "model_w2v = Word2Vec(sentences=all_tokens, vector_size=100, window=10, sg=1, hs=1, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert tokens to vectors, using zero vector for unknown words\n",
    "def tokens_to_vectors(tokens, model):\n",
    "    vectors = [model.wv[word] if word in model.wv else np.zeros((model.vector_size,)) for word in tokens]\n",
    "    return np.array(vectors)\n",
    "\n",
    "train['vectors'] = train['tokens'].apply(lambda tokens: tokens_to_vectors(tokens, model_w2v))\n",
    "test['vectors'] = test['tokens'].apply(lambda tokens: tokens_to_vectors(tokens, model_w2v))\n",
    "validation['vectors'] = validation['tokens'].apply(lambda tokens: tokens_to_vectors(tokens, model_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum sequence length to use for padding\n",
    "max_seq_length = max(train['vectors'].apply(len).max(),test['vectors'].apply(len).max(),validation['vectors'].apply(len).max())\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(train['vectors'].tolist(), maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "X_test_padded = pad_sequences(test['vectors'].tolist(), maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "X_val_padded = pad_sequences(validation['vectors'].tolist(), maxlen=max_seq_length, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501aef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "le= LabelEncoder()\n",
    "train['label']= le.fit_transform(train['sentiment'])\n",
    "test['label'] = le.transform(test['sentiment'])\n",
    "validation['label'] = le.transform(validation['sentiment'])\n",
    "\n",
    "y_train = train['label'].values\n",
    "y_test= test['label'].values\n",
    "y_val = validation['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb571011",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['textID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1725468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.drop(columns=['textID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.drop(columns=['textID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81068412",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment in np.unique(y_train):\n",
    "    sentiment_label = le.inverse_transform([sentiment])[0]\n",
    "    sentiment_indices = np.where(y_train == sentiment)[0]\n",
    "    print(f\"Train - {sentiment_label}: {len(sentiment_indices)}\")\n",
    "print(\"\")\n",
    "\n",
    "for sentiment in np.unique(y_val):\n",
    "    sentiment_label = le.inverse_transform([sentiment])[0]\n",
    "    sentiment_indices = np.where(y_val == sentiment)[0]\n",
    "    print(f\"Validation - {sentiment_label}: {len(sentiment_indices)}\")\n",
    "print(\"\")\n",
    "\n",
    "for sentiment in np.unique(y_test):\n",
    "    sentiment_label = le.inverse_transform([sentiment])[0]\n",
    "    sentiment_indices = np.where(y_test == sentiment)[0]\n",
    "    print(f\"Test - {sentiment_label}: {len(sentiment_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafda6fa",
   "metadata": {},
   "source": [
    "# Most common words Sentiments Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee02f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['list'] = train['text'].apply(preprocess_text)\n",
    "Positive_sent = train[train['sentiment']=='positive']\n",
    "Negative_sent = train[train['sentiment']=='negative']\n",
    "Neutral_sent = train[train['sentiment']=='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Most common positive words\n",
    "top = Counter([item for sublist in Positive_sent['list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(20))\n",
    "temp_positive = temp_positive.iloc[1:,:]\n",
    "temp_positive.columns = ['Common positive words','count']\n",
    "temp_positive.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55068803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common negative words\n",
    "\n",
    "top = Counter([item for sublist in Negative_sent['list'] for item in sublist])\n",
    "temp_negative = pd.DataFrame(top.most_common(20))\n",
    "temp_negative = temp_negative.iloc[1:,:]\n",
    "temp_negative.columns = ['Common negative words','count']\n",
    "temp_negative.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common neutral words\n",
    "\n",
    "top = Counter([item for sublist in Neutral_sent['list'] for item in sublist])\n",
    "temp_neutral = pd.DataFrame(top.most_common(20))\n",
    "temp_neutral = temp_neutral.loc[1:,:]\n",
    "temp_neutral.columns = ['Common neutral words','count']\n",
    "temp_neutral.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410b99",
   "metadata": {},
   "source": [
    "# Unique Words in each Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9327a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = [word for word_list in train['list'] for word in word_list]\n",
    "\n",
    "def words_unique(sentiment,numwords,raw_words):\n",
    "    allother = []\n",
    "    for item in train[train.sentiment != sentiment]['list']:\n",
    "        for word in item:\n",
    "            allother .append(word)\n",
    "    allother  = list(set(allother ))\n",
    "    \n",
    "    specificnonly = [x for x in raw_text if x not in allother]\n",
    "    \n",
    "    mycounter = Counter()\n",
    "    \n",
    "    for item in train[train.sentiment == sentiment]['list']:\n",
    "        for word in item:\n",
    "            mycounter[word] += 1\n",
    "    keep = list(specificnonly)\n",
    "    \n",
    "    for word in list(mycounter):\n",
    "        if word not in keep:\n",
    "            del mycounter[word]\n",
    "    \n",
    "    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n",
    "    \n",
    "    return Unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93154e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Positive= words_unique('positive', 10, raw_text)\n",
    "print(\"Top 10 unique positive words:\")\n",
    "Unique_Positive.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Negative= words_unique('negative', 10, raw_text)\n",
    "print(\"Top 10 unique negative words:\")\n",
    "Unique_Negative.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Neutral= words_unique('neutral', 10, raw_text)\n",
    "print(\"Top 10 unique neutral words:\")\n",
    "Unique_Neutral.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251f2b1",
   "metadata": {},
   "source": [
    "# Parameter adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dense, Conv1D, Activation, GlobalMaxPooling1D, Concatenate, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "# class MultiHeadAttention(layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, **kwargs):\n",
    "#         super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "#         self.num_heads = num_heads\n",
    "#         self.embed_dim = embed_dim\n",
    "#         if embed_dim % num_heads != 0:\n",
    "#             raise ValueError(\"embedding dimension must be divisible by number of heads \")\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "#         self.query = Dense(embed_dim)\n",
    "#         self.key = Dense(embed_dim)\n",
    "#         self.value = Dense(embed_dim)\n",
    "#         self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'num_heads': self.num_heads,\n",
    "#             'embed_dim': self.embed_dim\n",
    "#         })\n",
    "#         return config\n",
    "    \n",
    "#     def attention(self, query, key, value):\n",
    "#         score = tf.matmul(query, key, transpose_b=True)\n",
    "#         dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "#         scaled_score = score / tf.math.sqrt(dim_key)\n",
    "#         weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "#         output = tf.matmul(weights, value)\n",
    "#         return output, weights\n",
    "\n",
    "#     def separate_heads(self, x, batch_size):\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "#         # Linearly project the queries, keys, and values\n",
    "#         query = self.query(inputs) \n",
    "#         key = self.key(inputs)\n",
    "#         value = self.value(inputs) \n",
    "        \n",
    "#         # Split into multiple heads (batch_size, num_heads, max_seq_length, head_dim)\n",
    "#         query = self.separate_heads(query, batch_size) \n",
    "#         key = self.separate_heads(key, batch_size) \n",
    "#         value = self.separate_heads(value, batch_size) \n",
    "\n",
    "#         # Calculate attention scores\n",
    "#         attention, weights = self.attention(query, key, value)\n",
    "#         attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "#         concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "#         # Linearly combine the heads\n",
    "#         output = self.combine_heads(concat_attention)\n",
    "#         return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 150\n",
    "# weight = {0: 1.18, 1: 0.83, 2: 1.06}\n",
    "\n",
    "# filters = [32, 64, 128]\n",
    "# l2_param = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "# drop_out = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# all_train, all_val = list(), list()\n",
    "# embedding_dim = model_w2v.vector_size\n",
    "# input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "    \n",
    "# for param1, param2, param3 in itertools.product(filters, l2_param, drop_out):\n",
    "#     attention_out, _ = MultiHeadAttention(embed_dim=embedding_dim, num_heads=4)(input_layer)\n",
    "\n",
    "#     # TextCNN\n",
    "#     filter_sizes = [3, 4, 5]\n",
    "#     pool_outputs = []\n",
    "\n",
    "#     for filter_size in filter_sizes:\n",
    "#         conv = Conv1D(filters=param1, kernel_size=filter_size, padding='same',\n",
    "#                       kernel_initializer='he_normal',kernel_regularizer=l2(param2),\n",
    "#                       data_format='channels_last', use_bias=True)(attention_out)\n",
    "#         act = Activation('relu')(conv)\n",
    "#         norm = BatchNormalization()(act)\n",
    "#         max_pool = GlobalMaxPooling1D(data_format='channels_last')(conv)\n",
    "#         pool_outputs.append(max_pool)\n",
    "\n",
    "#     concatenated = Concatenate()(pool_outputs)\n",
    "#     dense = Dense(128, activation='relu')(concatenated)\n",
    "#     dropout = Dropout(param3)(dense)\n",
    "#     output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output)\n",
    "#     model.compile(optimizer= Adam(learning_rate=3e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.fit(X_train_padded, \n",
    "#               y_train, \n",
    "#               validation_data=(X_val_padded, y_val), \n",
    "#               epochs=30, \n",
    "#               class_weight = weight, \n",
    "#               batch_size=32, \n",
    "#               verbose=0)\n",
    "\n",
    "#     _, train_acc = model.evaluate(X_train_padded, y_train, verbose=0)\n",
    "#     _, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "    \n",
    "#     print('Param1: %f, Param2: %f, Param3: %f, Train: %.3f, Validation: %.3f' % (param1, param2, param3, train_acc, val_acc))\n",
    "#     all_train.append(train_acc)\n",
    "#     all_val.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c327fa7",
   "metadata": {},
   "source": [
    "# 1. Find Best L1/L2 Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae256d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 150\n",
    "# weight = {0: 1.18, 1: 0.83, 2: 1.06}\n",
    "# values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "# all_train, all_val = list(), list()\n",
    "\n",
    "# embedding_dim = model_w2v.vector_size\n",
    "# input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "\n",
    "# for param in values:\n",
    "#     attention_out, _ = MultiHeadAttention(embed_dim=embedding_dim, num_heads=4)(input_layer)\n",
    "\n",
    "#     # TextCNN\n",
    "#     filter_sizes = [3, 4, 5]\n",
    "#     pool_outputs = []\n",
    "\n",
    "#     for filter_size in filter_sizes:\n",
    "#         conv = Conv1D(filters=128, kernel_size=filter_size, padding='same',\n",
    "#                       kernel_initializer='he_normal',kernel_regularizer=l2(param),\n",
    "#                       data_format='channels_last', use_bias=True)(attention_out)\n",
    "#         act = Activation('relu')(conv)\n",
    "#         norm = BatchNormalization()(act)\n",
    "#         max_pool = GlobalMaxPooling1D(data_format='channels_last')(conv)\n",
    "#         pool_outputs.append(max_pool)\n",
    "\n",
    "#     concatenated = Concatenate()(pool_outputs)\n",
    "#     dense = Dense(256, activation='relu')(concatenated)\n",
    "#     dropout = Dropout(0.5)(dense)\n",
    "#     output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output)\n",
    "#     model.compile(optimizer= Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.fit(X_train_padded, \n",
    "#               y_train, \n",
    "#               validation_data=(X_val_padded, y_val), \n",
    "#               epochs=30, \n",
    "#               class_weight = weight, \n",
    "#               batch_size=32, \n",
    "#               verbose=0)\n",
    "\n",
    "#     _, train_acc = model.evaluate(X_train_padded, y_train, verbose=0)\n",
    "#     _, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "    \n",
    "#     print('Param: %f, Train: %.3f, Validation: %.3f' % (param, train_acc, val_acc))\n",
    "#     all_train.append(train_acc)\n",
    "#     all_val.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162acb2",
   "metadata": {},
   "source": [
    "# 2. Find Best Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f877ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 150\n",
    "# weight = {0: 1.18, 1: 0.83, 2: 1.06}\n",
    "# values = [0.2, 0.3, 0.4, 0.5]\n",
    "# all_train, all_val = list(), list()\n",
    "\n",
    "# embedding_dim = model_w2v.vector_size\n",
    "# input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "\n",
    "# for param in values:\n",
    "#     attention_out, _ = MultiHeadAttention(embed_dim=embedding_dim, num_heads=4)(input_layer)\n",
    "\n",
    "#     # TextCNN\n",
    "#     filter_sizes = [3, 4, 5]\n",
    "#     pool_outputs = []\n",
    "\n",
    "#     for filter_size in filter_sizes:\n",
    "#         conv = Conv1D(filters=128, kernel_size=filter_size, padding='same',\n",
    "#                       kernel_initializer='he_normal',kernel_regularizer=l2(1e-2),\n",
    "#                       data_format='channels_last', use_bias=True)(attention_out)\n",
    "#         act = Activation('relu')(conv)\n",
    "#         norm = BatchNormalization()(act)\n",
    "#         max_pool = GlobalMaxPooling1D(data_format='channels_last')(conv)\n",
    "#         pool_outputs.append(max_pool)\n",
    "\n",
    "#     concatenated = Concatenate()(pool_outputs)\n",
    "#     dense = Dense(256, activation='relu')(concatenated)\n",
    "#     dropout = Dropout(param)(dense)\n",
    "#     output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output)\n",
    "#     model.compile(optimizer= Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.fit(X_train_padded, \n",
    "#               y_train, \n",
    "#               validation_data=(X_val_padded, y_val), \n",
    "#               epochs=30, \n",
    "#               class_weight = weight, \n",
    "#               batch_size=32, \n",
    "#               verbose=0)\n",
    "\n",
    "#     _, train_acc = model.evaluate(X_train_padded, y_train, verbose=0)\n",
    "#     _, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "    \n",
    "#     print('Param: %f, Train: %.3f, Validation: %.3f' % (param, train_acc, val_acc))\n",
    "#     all_train.append(train_acc)\n",
    "#     all_val.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d00472",
   "metadata": {},
   "source": [
    "# 3. Find Best Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 150\n",
    "# weight = {0: 1.18, 1: 0.83, 2: 1.06}\n",
    "# values = [32, 64, 128, 256]\n",
    "# all_train, all_val = list(), list()\n",
    "\n",
    "# embedding_dim = model_w2v.vector_size\n",
    "# input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "\n",
    "# for param in values:\n",
    "#     attention_out, _ = MultiHeadAttention(embed_dim=embedding_dim, num_heads=4)(input_layer)\n",
    "\n",
    "#     # TextCNN\n",
    "#     filter_sizes = [3, 4, 5]\n",
    "#     pool_outputs = []\n",
    "\n",
    "#     for filter_size in filter_sizes:\n",
    "#         conv = Conv1D(filters=param, kernel_size=filter_size, padding='same',\n",
    "#                       kernel_initializer='he_normal',kernel_regularizer=l2(1e-2),\n",
    "#                       data_format='channels_last', use_bias=True)(attention_out)\n",
    "#         act = Activation('relu')(conv)\n",
    "#         norm = BatchNormalization()(act)\n",
    "#         max_pool = GlobalMaxPooling1D(data_format='channels_last')(conv)\n",
    "#         pool_outputs.append(max_pool)\n",
    "\n",
    "#     concatenated = Concatenate()(pool_outputs)\n",
    "#     dense = Dense(256, activation='relu')(concatenated)\n",
    "#     dropout = Dropout(0.5)(dense)\n",
    "#     output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output)\n",
    "#     model.compile(optimizer= Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.fit(X_train_padded, \n",
    "#               y_train, \n",
    "#               validation_data=(X_val_padded, y_val), \n",
    "#               epochs=30, \n",
    "#               class_weight = weight, \n",
    "#               batch_size=32, \n",
    "#               verbose=0)\n",
    "\n",
    "#     _, train_acc = model.evaluate(X_train_padded, y_train, verbose=0)\n",
    "#     _, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "    \n",
    "#     print('Param: %f, Train: %.3f, Validation: %.3f' % (param, train_acc, val_acc))\n",
    "#     all_train.append(train_acc)\n",
    "#     all_val.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e4ded",
   "metadata": {},
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0132b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Activation, GlobalMaxPooling1D, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "embedding_dim = model_w2v.vector_size\n",
    "input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"embedding dimension must be divisible by number of heads \")\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.query = Dense(embed_dim)\n",
    "        self.key = Dense(embed_dim)\n",
    "        self.value = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'embed_dim': self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Linearly project the queries, keys, and values\n",
    "        query = self.query(inputs) \n",
    "        key = self.key(inputs)\n",
    "        value = self.value(inputs) \n",
    "        \n",
    "        # Split into multiple heads (batch_size, num_heads, max_seq_length, head_dim)\n",
    "        query = self.separate_heads(query, batch_size) \n",
    "        key = self.separate_heads(key, batch_size) \n",
    "        value = self.separate_heads(value, batch_size) \n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "        # Linearly combine the heads\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output, weights\n",
    "\n",
    "attention_out, attention_weights = MultiHeadAttention(embed_dim=embedding_dim, num_heads=4)(input_layer)\n",
    "\n",
    "# TextCNN\n",
    "l2_reg = l2(l2=0.1)\n",
    "filter_sizes = [3, 4, 5]\n",
    "pool_outputs = []\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(filters=128, kernel_size=filter_size, padding='same',\n",
    "                  kernel_initializer='he_normal',kernel_regularizer=l2_reg,\n",
    "                  data_format='channels_last',use_bias=True)(attention_out)\n",
    "    act = Activation('relu')(conv)\n",
    "    norm = BatchNormalization()(act)\n",
    "    max_pool = GlobalMaxPooling1D(data_format='channels_last')(conv)\n",
    "    pool_outputs.append(max_pool)\n",
    "    \n",
    "concatenated = Concatenate()(pool_outputs)\n",
    "dense = Dense(256, activation='relu')(concatenated)\n",
    "dropout = Dropout(0.2)(dense)\n",
    "output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ba3ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# initial_lr = 0.001\n",
    "# lr_schedule = ExponentialDecay(\n",
    "#     initial_lr,\n",
    "#     decay_steps=35, \n",
    "#     decay_rate=0.9\n",
    "# )\n",
    "\n",
    "\n",
    "weight = {0: 1.18, 1: 0.83, 2: 1.06}\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=3e-4),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    validation_data=(X_val_padded, y_val), \n",
    "    epochs=50, \n",
    "    class_weight = weight,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ad6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='#6c8ebf')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#82B366')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='#6c8ebf')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='#82B366')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28594a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('cnn.keras', custom_objects={'MultiHeadAttention': MultiHeadAttention})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e998c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd099e",
   "metadata": {},
   "source": [
    "# LiIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5925eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('cnn.keras', custom_objects={'MultiHeadAttention': MultiHeadAttention})\n",
    "unique_classes = train['sentiment'].unique()\n",
    "class_names = unique_classes.tolist() \n",
    "\n",
    "def predict_proba(texts):\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    vectors = [tokens_to_vectors(text, model_w2v) for text in preprocessed_texts]\n",
    "    vectors_padded = pad_sequences(vectors, maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "    return model.predict(vectors_padded)\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(train.iloc[10]['text'], predict_proba, num_features=3, top_labels=1)\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead0bba",
   "metadata": {},
   "source": [
    "# Visualize the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce14884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "attention_model = Model(inputs=model.input, outputs=[model.output, attention_weights])\n",
    "\n",
    "def visualize_attention_weights(sentence, attention_model):\n",
    "    tokens = preprocess_text(sentence)\n",
    "    print(f\"Tokens after preprocessing: {tokens}\")\n",
    "    \n",
    "    vectors = tokens_to_vectors(tokens, model_w2v)\n",
    "    vectors_padded = pad_sequences([vectors], maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "    \n",
    "    prediction, attention_weights = attention_model.predict(vectors_padded)\n",
    "    attention_weights = attention_weights.flatten()\n",
    "    attention_weights = attention_weights[:len(tokens)]  \n",
    "    \n",
    "    plt.figure(figsize=(len(tokens), 1))\n",
    "    sns.heatmap([attention_weights], annot=True, cmap='Blues', cbar=False, xticklabels=tokens)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Randomly select two texts\n",
    "random_indices = np.random.choice(test.index, size=2, replace=False)\n",
    "sentences = test['text'].loc[random_indices]\n",
    "\n",
    "for sentence in sentences:\n",
    "    visualize_attention_weights(sentence, attention_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd389d",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbee9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_train = train['label'].values\n",
    "# model_attention_output = Model(inputs=model.input, outputs=attention_out)\n",
    "attention_output = attention_model.predict(X_train_padded)\n",
    "\n",
    "# Perform PCA on the features to reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "# reduced_attention_output = pca.fit_transform(attention_output.reshape(attention_output.shape[0], -1))\n",
    "reduced_attention_output = pca.fit_transform(attention_output)\n",
    "\n",
    "# Plotting the features after the attention layer\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_attention_output[:, 0], reduced_attention_output[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title(\"Features Visualization after Attention Mechanism\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "model_dense_output = Model(inputs=model.input, outputs=dense)\n",
    "dense_output = model_dense_output.predict(X_train_padded)\n",
    "\n",
    "# Perform PCA on these features as well\n",
    "reduced_dense_output = pca.fit_transform(dense_output)\n",
    "\n",
    "# Plotting the features before the softmax layer\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_dense_output[:, 0], reduced_dense_output[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title(\"Features Visualization before Softmax Layer\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f663add",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645728df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model_attention_output = Model(inputs=model.input, outputs=attention_out)\n",
    "attention_output = model_attention_output.predict(X_train_padded)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Perform t-SNE on the attention mechanism output\n",
    "tsne_attention_output = tsne.fit_transform(attention_output.reshape(attention_output.shape[0], -1))\n",
    "\n",
    "# Plotting the features after the attention layer\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_attention_output[:, 0], tsne_attention_output[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title(\"Features Visualization after Attention Mechanism\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "model_dense_output = Model(inputs=model.input, outputs=dense)\n",
    "dense_output = model_dense_output.predict(X_train_padded)\n",
    "\n",
    "# Perform t-SNE on the output before the softmax layer\n",
    "tsne_dense_output = tsne.fit_transform(dense_output)\n",
    "\n",
    "# Plotting the features before the softmax layer\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_dense_output[:, 0], tsne_dense_output[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title(\"Features Visualization before Softmax Layer\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19184b5",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict classes for test set\n",
    "y_pred = np.argmax(model.predict(X_test_padded), axis=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix with sentiment labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5508932",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebe745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert predicted labels to a DataFrame\n",
    "df_results = pd.DataFrame({'Text':test['text'],\n",
    "                           'True_Label': le.inverse_transform(y_test),\n",
    "                           'Predicted_Label': le.inverse_transform(y_pred)})\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee7e02",
   "metadata": {},
   "source": [
    "# ROC-AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27007d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Convert y_test to one-hot encoded format if it's not already\n",
    "if len(y_test.shape) == 1:\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "# Calculate the ROC AUC Score\n",
    "y_prob = model.predict(X_test_padded)\n",
    "roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_ovr:0.2f}\")\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = y_test.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "colors = cycle(['#6c8ebf', '#B3B3B3', '#82B366'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='{0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b28679",
   "metadata": {},
   "source": [
    "# PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Compute Precision-Recall curve for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_prob[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_prob[:, i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "colors = cycle(['#6c8ebf', '#B3B3B3', '#82B366'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='{0} (AP = {1:0.2f})'.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75de712",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "posText = train[train['sentiment'] == 'positive']['text']\n",
    "negText = train[train['sentiment'] == 'negative']['text']\n",
    "neuText = train[train['sentiment'] == 'neutral']['text']\n",
    "\n",
    "# Tokenize and extract words for each sentiment category\n",
    "posWord = [word.lower() for text in posText for word in word_tokenize(str(text))]\n",
    "negWord = [word.lower() for text in negText for word in word_tokenize(str(text))]\n",
    "neuWord = [word.lower() for text in neuText for word in word_tokenize(str(text))]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "positive = [word for word in posWord if word not in stop_words]\n",
    "negative = [word for word in negWord if word not in stop_words]\n",
    "neutral = [word for word in neuWord if word not in stop_words]\n",
    "\n",
    "def plot_word_cloud(words, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',colormap= 'coolwarm').generate(' '.join(words))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_word_cloud(positive, 'Positive Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60030926",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(negative, 'Negative Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58aeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(neutral, 'Neutral Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b48775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = {\n",
    "    \"positive\": posWord,\n",
    "    \"negative\": negWord,\n",
    "    \"neutral\": neuWord    \n",
    "}\n",
    "\n",
    "def get_average_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Calculate average vectors for sentiment categories\n",
    "average_vectors = {sentiment: get_average_vector(words, model_w2v) for sentiment, words in sentiment_lexicon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f496a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter text: \")\n",
    "user_tokens = preprocess_text(user_input)\n",
    "\n",
    "# Convert tokens to vectors using Word2Vec model\n",
    "user_vectors = get_average_vector(user_tokens, model_w2v).reshape(1, -1)\n",
    "\n",
    "# Pad sequences\n",
    "padded_user_input = pad_sequences([user_vectors], maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "\n",
    "# Perform prediction using the model\n",
    "prediction = model.predict(padded_user_input)\n",
    "\n",
    "predicted_label = np.argmax(prediction[0], axis=-1)\n",
    "pred_label = le.inverse_transform([predicted_label])\n",
    "\n",
    "print(\"Predicted label:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb07a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
