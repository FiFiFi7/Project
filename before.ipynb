{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9444ec",
   "metadata": {
    "id": "6e9444ec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948e551",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f948e551",
    "outputId": "af9e4df1-65c3-45f5-9db1-d605a3a7ee57"
   },
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "df_train = pd.read_csv(\"./dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"./dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41af654",
   "metadata": {
    "id": "a41af654"
   },
   "outputs": [],
   "source": [
    "# Basic text cleaning and tokenization with Jieba\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    # Tokenization with Jieba\n",
    "    tokens = list(jieba.cut(text))\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a28cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b5a28cd",
    "outputId": "68ecb498-b80a-4f87-8baf-29d22271c4a5"
   },
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "df_train['text'] = df_train['text'].fillna('')\n",
    "df_test['text'] = df_test['text'].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['tokens'] = df_train['text'].apply(preprocess_text)\n",
    "df_test['tokens'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41548bb",
   "metadata": {
    "id": "e41548bb"
   },
   "outputs": [],
   "source": [
    "# Concatenate tokens for Word2Vec training\n",
    "all_tokens = pd.concat([df_train['tokens'], df_test['tokens']], axis=0)\n",
    "\n",
    "# Train a Word2Vec model or load a pre-trained model\n",
    "model_w2v = Word2Vec(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1262355",
   "metadata": {
    "id": "e1262355"
   },
   "outputs": [],
   "source": [
    "# Function to convert tokens to vectors, using zero vector for unknown words\n",
    "def tokens_to_vectors(tokens, model):\n",
    "    vectors = [model.wv[word] if word in model.wv else np.zeros((model.vector_size,)) for word in tokens]\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Convert train and test tokens to vectors\n",
    "df_train['vectors'] = df_train['tokens'].apply(lambda tokens: tokens_to_vectors(tokens, model_w2v))\n",
    "df_test['vectors'] = df_test['tokens'].apply(lambda tokens: tokens_to_vectors(tokens, model_w2v))\n",
    "\n",
    "# Find the maximum sequence length to use for padding\n",
    "max_seq_length = max(df_train['vectors'].apply(len).max(), df_test['vectors'].apply(len).max())\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(df_train['vectors'].tolist(), maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "X_test_padded = pad_sequences(df_test['vectors'].tolist(), maxlen=max_seq_length, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c85180",
   "metadata": {
    "id": "37c85180"
   },
   "outputs": [],
   "source": [
    "# Convert sentiment labels to numerical values\n",
    "label_mapping = {'negative': 0, 'positive': 1, 'neutral': 2}\n",
    "df_train['label'] = df_train['sentiment'].map(label_mapping)\n",
    "df_test['label'] = df_test['sentiment'].map(label_mapping)\n",
    "\n",
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14d406",
   "metadata": {
    "id": "5b14d406"
   },
   "outputs": [],
   "source": [
    "# Split dataset into training, validation, and test sets\n",
    "X_train_padded, X_temp, y_train, y_temp = train_test_split(X_train_padded, y_train, test_size=0.4, random_state=42)\n",
    "X_val_padded, X_test_padded, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb07d06",
   "metadata": {
    "id": "1cb07d06"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483ad49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4483ad49",
    "outputId": "031547c6-3703-4d03-a4b1-212f33055e2f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = model_w2v.vector_size\n",
    "input_layer = Input(shape=(max_seq_length, embedding_dim))\n",
    "attention_out = MultiHeadAttention(num_heads=2, key_dim=embedding_dim, value_dim=embedding_dim)(query=input_layer, key=input_layer, value=input_layer)\n",
    "\n",
    "# TextCNN\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu')(attention_out)\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters=100, kernel_size=4, activation='relu')(attention_out)\n",
    "pool2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters=100, kernel_size=5, activation='relu')(attention_out)\n",
    "pool3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "concatenated = Concatenate()([pool1, pool2, pool3])\n",
    "\n",
    "dropout_layer = Dropout(0.5)(concatenated)\n",
    "\n",
    "# Fully connected layer with batch normalization\n",
    "dense = Dense(units=256, activation='relu', kernel_regularizer=l2(0.01))(dropout_layer)\n",
    "batch_norm = BatchNormalization()(dense)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(units=3, activation='softmax')(batch_norm)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train, epochs=50, batch_size=32, validation_data=(X_val_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ad6e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "b28ad6e3",
    "outputId": "31e1d8b0-e4b3-4746-c60a-50c0f3e2a7b2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='#6c8ebf')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#82B366')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='#6c8ebf')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='#82B366')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27007d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "27007d66",
    "outputId": "f5aefaa5-d42c-4328-d744-8153d5c65565"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "if len(y_test.shape) == 1:\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "label_map = {0: 'negative', 1: 'positive', 2: 'neutral'}\n",
    "\n",
    "y_prob = model.predict(X_test_padded)\n",
    "roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_ovr:0.2f}\")\n",
    "\n",
    "n_classes = y_test.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "colors = cycle(['#6c8ebf', '#B3B3B3', '#82B366'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='{0} (AUC = {1:0.2f})'.format(label_map[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da201281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "da201281",
    "outputId": "516fc33a-c0e3-48e2-da03-68ce5cf4cd3b"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test_padded), axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=[label_map[i] for i in range(n_classes)], \n",
    "            yticklabels=[label_map[i] for i in range(n_classes)])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5a9bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "e0b5a9bb",
    "outputId": "6ae03608-97c1-465c-b136-61d5a2559ae3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_prob[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_prob[:, i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "colors = cycle(['#6c8ebf', '#B3B3B3', '#82B366'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='{0} (AP = {1:0.2f})'.format(label_map[i], average_precision[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1120f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test_padded), axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=label_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676fd46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "9676fd46",
    "outputId": "09483ac5-4aaf-4153-8041-fd63c561331b"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "posText = df_train[df_train['sentiment'] == 'positive']['selected_text']\n",
    "negText = df_train[df_train['sentiment'] == 'negative']['selected_text']\n",
    "neuText = df_train[df_train['sentiment'] == 'neutral']['selected_text']\n",
    "\n",
    "posWord = [word.lower() for text in posText for word in word_tokenize(str(text))]\n",
    "negWord = [word.lower() for text in negText for word in word_tokenize(str(text))]\n",
    "neuWord = [word.lower() for text in neuText for word in word_tokenize(str(text))]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "positive = [word for word in posWord if word not in stop_words]\n",
    "negative = [word for word in negWord if word not in stop_words]\n",
    "neutral = [word for word in neuWord if word not in stop_words]\n",
    "\n",
    "def plot_word_cloud(words, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',colormap= 'summer').generate(' '.join(words))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_word_cloud(positive, 'Positive Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0P7biYBfYmLY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "0P7biYBfYmLY",
    "outputId": "8556662f-4dc5-427e-8899-ae83ef674240"
   },
   "outputs": [],
   "source": [
    "plot_word_cloud(negative, 'Negative Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OZN5rEceYo7t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "OZN5rEceYo7t",
    "outputId": "7bf41f84-de8c-4ab6-fbe2-dafa77582f23"
   },
   "outputs": [],
   "source": [
    "plot_word_cloud(neutral, 'Neutral Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9e6f7",
   "metadata": {
    "id": "6bf9e6f7"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=[output_layer, attention_out])\n",
    "prediction, attention_weights = model.predict(X_test_padded)\n",
    "attention_sample = attention_weights[0]\n",
    "\n",
    "attention_avg = np.mean(attention_sample, axis=0)\n",
    "\n",
    "top_k_indices = np.argsort(attention_avg)[-5:]  \n",
    "def highlight_keywords(text_tokens, top_indices):\n",
    "    highlighted_text = \"\"\n",
    "    for idx, token in enumerate(text_tokens):\n",
    "        if idx in top_indices:\n",
    "            highlighted_text += \"**{}** \".format(token) \n",
    "        else:\n",
    "            highlighted_text += \"{} \".format(token)\n",
    "    return highlighted_text\n",
    "\n",
    "highlighted_text = highlight_keywords(df_train['tokens'], top_k_indices)\n",
    "print(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dad248",
   "metadata": {
    "id": "c5dad248"
   },
   "outputs": [],
   "source": [
    "sentiment_lexicon = {\n",
    "    \"positive\": positive_words,\n",
    "    \"negative\": negative_words,\n",
    "    \"neutral\": neutral_words\n",
    "}\n",
    "\n",
    "def get_average_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "average_vectors = {\n",
    "    sentiment: get_average_vector(words, model_w2v) for sentiment, words in sentiment_lexicon.items()\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
